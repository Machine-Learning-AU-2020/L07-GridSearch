{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITMAL Exercise\n",
    "\n",
    "REVISIONS| |\n",
    "---------| |\n",
    "2018-0301| CEF, initial.\n",
    "2018-0305| CEF, updated.\n",
    "2018-0306| CEF, updated and spell checked.\n",
    "2018-0306| CEF, major overhaul of functions.\n",
    "2018-0306| CEF, fixed problem with MNIST load and Keras.\n",
    "2018-0307| CEF, modified report functions and changed Qc+d.\n",
    "2018-0311| CEF, updated Qd.\n",
    "2018-0312| CEF, added grid and random search figs and added bullets to Qd.\n",
    "2018-0313| CEF, fixed SVC and gamma issue, and changed dataload to be in fetchmode (non-keras).\n",
    "2019-1015| CEF, updated for ITMAL E19\n",
    "2019-1019| CEF, minor text update.\n",
    "2019-1023| CEF, changed demo model i Qd) from MLPClassifier to SVC.\n",
    "2020-0314| CEF, updated to ITMAL F20.\n",
    "\n",
    "## Hyperparameters and Gridsearch \n",
    "\n",
    "When instantiating a Scikit-learn model in python most or all constructor parameters have _default_ values. These values are not part of the internal model and are hence called ___hyperparametes___---in contrast to _normal_ model parameters, say the weights, $\\mathbf w$, for an `SGD` model.\n",
    "\n",
    "### Manual Tuning Hyperparameters\n",
    "\n",
    "Below is an example of the python constructor for the support-vector classifier `sklearn.svm.SVC`, with, say the `kernel` hyperparameter having the default value `'rbf'`. If you should choose, what would you set it to other than `'rbf'`? \n",
    "\n",
    "```python\n",
    "class sklearn.svm.SVC(\n",
    "    C=1.0, \n",
    "    kernel=’rbf’, \n",
    "    degree=3,\n",
    "    gamma=’auto_deprecated’, \n",
    "    coef0=0.0, \n",
    "    shrinking=True, \n",
    "    probability=False, \n",
    "    tol=0.001, \n",
    "    cache_size=200, \n",
    "    class_weight=None, \n",
    "    verbose=False, \n",
    "    max_iter=-1, \n",
    "    decision_function_shape=’ovr’, \n",
    "    random_state=None\n",
    "  )\n",
    "```  \n",
    "\n",
    "The default values might be sensible a general starting point, but for your data, you might want to optimize the hyperparameters to yield a better result. \n",
    "\n",
    "To be able to set `kernel` to a sensible value you need to go into the documentation for the `SVC` and understand what the kernel parameter represents, and what values it can be set to, and you need to understand the consequences of setting `kernel` to something different than the default...and the story repeats for every other hyperparameter!\n",
    "\n",
    "### Brute Force  Search\n",
    "\n",
    "An alternative to this structured, but time-consuming approach, is just to __brute-force__ a search of interesting hyperparameters, an choosing the 'best' parameters according to a fit-predict and some score, say 'f1'. \n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/F20_itmal/L09/Figs/gridsearch.png\" style=\"width:350px\">\n",
    "<small><em>\n",
    "    <center> Conceptual graphical view of grid search for two distinct hyperparameters. </center> \n",
    "    <center> Notice that you would normally search hyperparameters like `alpha` with an exponential range, say [0.01, 0.1, 1, 10] or similar.</center>\n",
    "</em></small>\n",
    "\n",
    "Now, you just pick out some hyperparameters, that you figure are important, set them to a suitable range, say\n",
    "\n",
    "```python\n",
    "    'kernel':('linear', 'rbf'), \n",
    "    'C':[1, 10]\n",
    "```\n",
    "and fire up a full (grid) search on this hyperparameter set, that will try out all combination of `kernel` and `C` for the model, and then prints the hyperparameter set with the highest score...\n",
    "\n",
    "The demo code below sets up some of our well known 'hello-world' data and run a _grid search_ on a particular model, here a _support-vector classifier_ (SVC)\n",
    "\n",
    "Other models and datasets  ('mnist', 'iris', 'moon') can also be examined.\n",
    "\n",
    "### Qa Explain GridSearchCV\n",
    "\n",
    "There are two code cells below: 1) function setup, 2) the actual grid-search.\n",
    "\n",
    "Review the code cells and write a __short__ summary. Mainly focus on __cell 2__, but dig into cell 1 if you find it interesting (notice the use of local-function, a nifty feature in python).\n",
    "  \n",
    "In detail, examine the lines:  \n",
    "  \n",
    "```python\n",
    "grid_tuned = GridSearchCV(model, tuning_parameters, ..\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "..\n",
    "FullReport(grid_tuned , X_test, y_test, time_gridsearch)\n",
    "```\n",
    "and write a short description of how the `GridSeachCV` works: explain how the search parameter set is created and the overall search mechanism is functioning (without going into to much detail).\n",
    "\n",
    "What role does the parameter `scoring='f1_micro'` play in the `GridSearchCV`, and what does `n_jobs=-1` mean? \n",
    "\n",
    "NOTICE: you need the dataloader module from `libitmal`, clone \n",
    "```\n",
    "> git clone https://cfrigaard@bitbucket.org/cfrigaard/itmal\n",
    "\n",
    "```\n",
    "or pull the GIT repository to get the latest version, and put `libitmal` into the python path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "OK(function setup, hope MNIST loads works, seem best if you got Keras or Tensorflow installed!)\n"
    }
   ],
   "source": [
    "# TODO: Qa, code review..cell 1) function setup\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"C:/Users/edwar/Desktop/itmal\")\n",
    "\n",
    "from time import time\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn import datasets\n",
    "\n",
    "from libitmal import dataloaders as itmaldataloaders # Needed for load of iris, moon and mnist\n",
    "\n",
    "currmode=\"N/A\" # GLOBAL var!\n",
    "\n",
    "def SearchReport(model): \n",
    "    \n",
    "    def GetBestModelCTOR(model, best_params):\n",
    "        def GetParams(best_params):\n",
    "            ret_str=\"\"          \n",
    "            for key in sorted(best_params):\n",
    "                value = best_params[key]\n",
    "                temp_str = \"'\" if str(type(value))==\"<class 'str'>\" else \"\"\n",
    "                if len(ret_str)>0:\n",
    "                    ret_str += ','\n",
    "                ret_str += f'{key}={temp_str}{value}{temp_str}'  \n",
    "            return ret_str          \n",
    "        try:\n",
    "            param_str = GetParams(best_params)\n",
    "            return type(model).__name__ + '(' + param_str + ')' \n",
    "        except:\n",
    "            return \"N/A(1)\"\n",
    "        \n",
    "    print(\"\\nBest model set found on train set:\")\n",
    "    print()\n",
    "    print(f\"\\tbest parameters={model.best_params_}\")\n",
    "    print(f\"\\tbest '{model.scoring}' score={model.best_score_}\")\n",
    "    print(f\"\\tbest index={model.best_index_}\")\n",
    "    print()\n",
    "    print(f\"Best estimator CTOR:\")\n",
    "    print(f\"\\t{model.best_estimator_}\")\n",
    "    print()\n",
    "    try:\n",
    "        print(f\"Grid scores ('{model.scoring}') on development set:\")\n",
    "        means = model.cv_results_['mean_test_score']\n",
    "        stds  = model.cv_results_['std_test_score']\n",
    "        i=0\n",
    "        for mean, std, params in zip(means, stds, model.cv_results_['params']):\n",
    "            print(\"\\t[%2d]: %0.3f (+/-%0.03f) for %r\" % (i, mean, std * 2, params))\n",
    "            i += 1\n",
    "    except:\n",
    "        print(\"WARNING: the random search do not provide means/stds\")\n",
    "    \n",
    "    global currmode                \n",
    "    assert \"f1_micro\"==str(model.scoring), f\"come on, we need to fix the scoring to be able to compare model-fits! Your scoreing={str(model.scoring)}...remember to add scoring='f1_micro' to the search\"   \n",
    "    return f\"best: dat={currmode}, score={model.best_score_:0.5f}, model={GetBestModelCTOR(model.estimator,model.best_params_)}\", model.best_estimator_ \n",
    "\n",
    "def ClassificationReport(model, X_test, y_test, target_names=None):\n",
    "    assert X_test.shape[0]==y_test.shape[0]\n",
    "    print(\"\\nDetailed classification report:\")\n",
    "    print(\"\\tThe model is trained on the full development set.\")\n",
    "    print(\"\\tThe scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, model.predict(X_test)                 \n",
    "    print(classification_report(y_true, y_pred, target_names))\n",
    "    print()\n",
    "    \n",
    "def FullReport(model, X_test, y_test, t):\n",
    "    print(f\"SEARCH TIME: {t:0.2f} sec\")\n",
    "    beststr, bestmodel = SearchReport(model)\n",
    "    ClassificationReport(model, X_test, y_test)    \n",
    "    print(f\"CTOR for best model: {bestmodel}\\n\")\n",
    "    print(f\"{beststr}\\n\")\n",
    "    return beststr, bestmodel\n",
    "    \n",
    "def LoadAndSetupData(mode, test_size=0.3):\n",
    "    assert test_size>=0.0 and test_size<=1.0\n",
    "    \n",
    "    def ShapeToString(Z):\n",
    "        n = Z.ndim\n",
    "        s = \"(\"\n",
    "        for i in range(n):\n",
    "            s += f\"{Z.shape[i]:5d}\"\n",
    "            if i+1!=n:\n",
    "                s += \";\"\n",
    "        return s+\")\"\n",
    "\n",
    "    global currmode\n",
    "    currmode=mode\n",
    "    print(f\"DATA: {currmode}..\")\n",
    "    \n",
    "    if mode=='moon':\n",
    "        X, y = itmaldataloaders.MOON_GetDataSet(n_samples=5000, noise=0.2)\n",
    "        itmaldataloaders.MOON_Plot(X, y)\n",
    "    elif mode=='mnist':\n",
    "        X, y = itmaldataloaders.MNIST_GetDataSet(load_mode=2)\n",
    "        if X.ndim==3:\n",
    "            X=np.reshape(X, (X.shape[0], -1))\n",
    "    elif mode=='iris':\n",
    "        X, y = itmaldataloaders.IRIS_GetDataSet()\n",
    "    else:\n",
    "        raise ValueError(f\"could not load data for that particular mode='{mode}', only 'moon'/'mnist'/'iris' supported\")\n",
    "        \n",
    "    print(f'  org. data:  X.shape      ={ShapeToString(X)}, y.shape      ={ShapeToString(y)}')\n",
    "\n",
    "    assert X.ndim==2\n",
    "    assert X.shape[0]==y.shape[0]\n",
    "    assert y.ndim==1 or (y.ndim==2 and y.shape[1]==0)    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=0, shuffle=True\n",
    "    )\n",
    "    \n",
    "    print(f'  train data: X_train.shape={ShapeToString(X_train)}, y_train.shape={ShapeToString(y_train)}')\n",
    "    print(f'  test data:  X_test.shape ={ShapeToString(X_test)}, y_test.shape ={ShapeToString(y_test)}')\n",
    "    print()\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "print('OK(function setup, hope MNIST loads works, seem best if you got Keras or Tensorflow installed!)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "DATA: iris..\n  org. data:  X.shape      =(  150;    4), y.shape      =(  150)\n  train data: X_train.shape=(  105;    4), y_train.shape=(  105)\n  test data:  X_test.shape =(   45;    4), y_test.shape =(   45)\n\nSEARCH TIME: 1.54 sec\n\nBest model set found on train set:\n\n\tbest parameters={'C': 1, 'kernel': 'linear'}\n\tbest 'f1_micro' score=0.9714285714285714\n\tbest index=0\n\nBest estimator CTOR:\n\tSVC(C=1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='linear',\n    max_iter=-1, probability=False, random_state=None, shrinking=True,\n    tol=0.001, verbose=False)\n\nGrid scores ('f1_micro') on development set:\n\t[ 0]: 0.971 (+/-0.047) for {'C': 1, 'kernel': 'linear'}\n\t[ 1]: 0.695 (+/-0.047) for {'C': 1, 'kernel': 'rbf'}\n\t[ 2]: 0.952 (+/-0.085) for {'C': 10, 'kernel': 'linear'}\n\t[ 3]: 0.924 (+/-0.097) for {'C': 10, 'kernel': 'rbf'}\n\nDetailed classification report:\n\tThe model is trained on the full development set.\n\tThe scores are computed on the full evaluation set.\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        16\n           1       1.00      0.94      0.97        18\n           2       0.92      1.00      0.96        11\n\n    accuracy                           0.98        45\n   macro avg       0.97      0.98      0.98        45\nweighted avg       0.98      0.98      0.98        45\n\n\nCTOR for best model: SVC(C=1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='linear',\n    max_iter=-1, probability=False, random_state=None, shrinking=True,\n    tol=0.001, verbose=False)\n\nbest: dat=iris, score=0.97143, model=SVC(C=1,kernel='linear')\n\nOK(grid-search)\n"
    }
   ],
   "source": [
    "# TODO: Qa, code review..cell 2) the actual grid-search\n",
    "\n",
    "# Setup data\n",
    "X_train, X_test, y_train, y_test = LoadAndSetupData('iris') # 'iris', 'moon', or 'mnist'\n",
    "\n",
    "# Setup search parameters\n",
    "model = svm.SVC(gamma=0.001) # NOTE: gamma=\"scale\" does not work in older Scikit-learn frameworks, \n",
    "                             # FIX:  replace with model = svm.SVC(gamma=0.001)\n",
    "\n",
    "tuning_parameters = {\n",
    "    'kernel':('linear', 'rbf'), \n",
    "    'C':[1, 10]\n",
    "}\n",
    "\n",
    "CV=5\n",
    "VERBOSE=0\n",
    "\n",
    "# Run GridSearchCV for the model\n",
    "start = time()\n",
    "grid_tuned = GridSearchCV(model, tuning_parameters, cv=CV, scoring='f1_micro', verbose=VERBOSE, n_jobs=-1, iid=True)\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "t = time()-start\n",
    "\n",
    "# Report result\n",
    "b0, m0= FullReport(grid_tuned , X_test, y_test, t)\n",
    "print('OK(grid-search)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qb Hyperparameter Grid Search using an SDG classifier\n",
    "\n",
    "Now, replace the `svm.SVC` model with an `SGDClassifier` and a suitable set of the hyperparameters for that model.\n",
    "\n",
    "You need at least four or five different hyperparameters from the `SGDClassifier` in the search-space before it begins to take considerable compute time doing the full grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "daptive', 'penalty': 'l2', 'power_t': 0.1}\n\t[6009]: 0.819 (+/-0.111) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.1, 'learning_rate': 'adaptive', 'penalty': 'l2', 'power_t': 0.2}\n\t[6010]: 0.819 (+/-0.111) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.1, 'learning_rate': 'adaptive', 'penalty': 'l2', 'power_t': 0.3}\n\t[6011]: 0.800 (+/-0.140) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.1, 'learning_rate': 'adaptive', 'penalty': 'l2', 'power_t': 0.4}\n\t[6012]: 0.810 (+/-0.085) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.1, 'learning_rate': 'adaptive', 'penalty': 'l2', 'power_t': 0.5}\n\t[6013]: 0.838 (+/-0.114) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.1, 'learning_rate': 'adaptive', 'penalty': 'l2', 'power_t': 0.6}\n\t[6014]: 0.800 (+/-0.071) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.1, 'learning_rate': 'adaptive', 'penalty': 'l2', 'power_t': 0.7}\n\t[6015]: 0.810 (+/-0.060) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.1, 'learning_rate': 'adaptive', 'penalty': 'l2', 'power_t': 0.8}\n\t[6016]: 0.667 (+/-0.120) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'constant', 'penalty': 'elasticnet', 'power_t': 0.1}\n\t[6017]: 0.857 (+/-0.283) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'constant', 'penalty': 'elasticnet', 'power_t': 0.2}\n\t[6018]: 0.714 (+/-0.104) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'constant', 'penalty': 'elasticnet', 'power_t': 0.3}\n\t[6019]: 0.724 (+/-0.185) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'constant', 'penalty': 'elasticnet', 'power_t': 0.4}\n\t[6020]: 0.724 (+/-0.285) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'constant', 'penalty': 'elasticnet', 'power_t': 0.5}\n\t[6021]: 0.676 (+/-0.071) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'constant', 'penalty': 'elasticnet', 'power_t': 0.6}\n\t[6022]: 0.781 (+/-0.214) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'constant', 'penalty': 'elasticnet', 'power_t': 0.7}\n\t[6023]: 0.648 (+/-0.114) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'constant', 'penalty': 'elasticnet', 'power_t': 0.8}\n\t[6024]: 0.829 (+/-0.273) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'constant', 'penalty': 'none', 'power_t': 0.1}\n\t[6025]: 0.819 (+/-0.251) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'constant', 'penalty': 'none', 'power_t': 0.2}\n\t[6026]: 0.838 (+/-0.230) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'constant', 'penalty': 'none', 'power_t': 0.3}\n\t[6027]: 0.733 (+/-0.047) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'constant', 'penalty': 'none', 'power_t': 0.4}\n\t[6028]: 0.733 (+/-0.177) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'constant', 'penalty': 'none', 'power_t': 0.5}\n\t[6029]: 0.705 (+/-0.185) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'constant', 'penalty': 'none', 'power_t': 0.6}\n\t[6030]: 0.762 (+/-0.200) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'constant', 'penalty': 'none', 'power_t': 0.7}\n\t[6031]: 0.705 (+/-0.338) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'constant', 'penalty': 'none', 'power_t': 0.8}\n\t[6032]: 0.600 (+/-0.280) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'constant', 'penalty': 'l1', 'power_t': 0.1}\n\t[6033]: 0.752 (+/-0.164) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'constant', 'penalty': 'l1', 'power_t': 0.2}\n\t[6034]: 0.667 (+/-0.085) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'constant', 'penalty': 'l1', 'power_t': 0.3}\n\t[6035]: 0.600 (+/-0.322) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'constant', 'penalty': 'l1', 'power_t': 0.4}\n\t[6036]: 0.705 (+/-0.321) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'constant', 'penalty': 'l1', 'power_t': 0.5}\n\t[6037]: 0.724 (+/-0.093) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'constant', 'penalty': 'l1', 'power_t': 0.6}\n\t[6038]: 0.762 (+/-0.225) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'constant', 'penalty': 'l1', 'power_t': 0.7}\n\t[6039]: 0.695 (+/-0.097) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'constant', 'penalty': 'l1', 'power_t': 0.8}\n\t[6040]: 0.514 (+/-0.315) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'constant', 'penalty': 'l2', 'power_t': 0.1}\n\t[6041]: 0.619 (+/-0.289) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'constant', 'penalty': 'l2', 'power_t': 0.2}\n\t[6042]: 0.533 (+/-0.406) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'constant', 'penalty': 'l2', 'power_t': 0.3}\n\t[6043]: 0.619 (+/-0.295) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'constant', 'penalty': 'l2', 'power_t': 0.4}\n\t[6044]: 0.629 (+/-0.212) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'constant', 'penalty': 'l2', 'power_t': 0.5}\n\t[6045]: 0.667 (+/-0.455) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'constant', 'penalty': 'l2', 'power_t': 0.6}\n\t[6046]: 0.714 (+/-0.120) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'constant', 'penalty': 'l2', 'power_t': 0.7}\n\t[6047]: 0.686 (+/-0.076) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'constant', 'penalty': 'l2', 'power_t': 0.8}\n\t[6048]: 0.752 (+/-0.164) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'elasticnet', 'power_t': 0.1}\n\t[6049]: 0.800 (+/-0.258) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'elasticnet', 'power_t': 0.2}\n\t[6050]: 0.733 (+/-0.129) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'elasticnet', 'power_t': 0.3}\n\t[6051]: 0.752 (+/-0.164) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'elasticnet', 'power_t': 0.4}\n\t[6052]: 0.743 (+/-0.129) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'elasticnet', 'power_t': 0.5}\n\t[6053]: 0.771 (+/-0.203) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'elasticnet', 'power_t': 0.6}\n\t[6054]: 0.752 (+/-0.140) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'elasticnet', 'power_t': 0.7}\n\t[6055]: 0.762 (+/-0.085) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'elasticnet', 'power_t': 0.8}\n\t[6056]: 0.981 (+/-0.047) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'none', 'power_t': 0.1}\n\t[6057]: 0.981 (+/-0.047) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'none', 'power_t': 0.2}\n\t[6058]: 0.962 (+/-0.071) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'none', 'power_t': 0.3}\n\t[6059]: 0.952 (+/-0.060) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'none', 'power_t': 0.4}\n\t[6060]: 0.971 (+/-0.047) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'none', 'power_t': 0.5}\n\t[6061]: 0.981 (+/-0.047) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'none', 'power_t': 0.6}\n\t[6062]: 0.971 (+/-0.076) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'none', 'power_t': 0.7}\n\t[6063]: 0.962 (+/-0.071) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'none', 'power_t': 0.8}\n\t[6064]: 0.790 (+/-0.214) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'l1', 'power_t': 0.1}\n\t[6065]: 0.800 (+/-0.140) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'l1', 'power_t': 0.2}\n\t[6066]: 0.819 (+/-0.152) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'l1', 'power_t': 0.3}\n\t[6067]: 0.829 (+/-0.076) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'l1', 'power_t': 0.4}\n\t[6068]: 0.733 (+/-0.097) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'l1', 'power_t': 0.5}\n\t[6069]: 0.762 (+/-0.190) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'l1', 'power_t': 0.6}\n\t[6070]: 0.810 (+/-0.135) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'l1', 'power_t': 0.7}\n\t[6071]: 0.790 (+/-0.245) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'l1', 'power_t': 0.8}\n\t[6072]: 0.752 (+/-0.152) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'l2', 'power_t': 0.1}\n\t[6073]: 0.771 (+/-0.140) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'l2', 'power_t': 0.2}\n\t[6074]: 0.771 (+/-0.140) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'l2', 'power_t': 0.3}\n\t[6075]: 0.819 (+/-0.185) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'l2', 'power_t': 0.4}\n\t[6076]: 0.800 (+/-0.203) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'l2', 'power_t': 0.5}\n\t[6077]: 0.771 (+/-0.071) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'l2', 'power_t': 0.6}\n\t[6078]: 0.867 (+/-0.203) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'l2', 'power_t': 0.7}\n\t[6079]: 0.819 (+/-0.126) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'l2', 'power_t': 0.8}\n\t[6080]: 0.810 (+/-0.248) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'invscaling', 'penalty': 'elasticnet', 'power_t': 0.1}\n\t[6081]: 0.705 (+/-0.126) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'invscaling', 'penalty': 'elasticnet', 'power_t': 0.2}\n\t[6082]: 0.724 (+/-0.140) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'invscaling', 'penalty': 'elasticnet', 'power_t': 0.3}\n\t[6083]: 0.695 (+/-0.047) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'invscaling', 'penalty': 'elasticnet', 'power_t': 0.4}\n\t[6084]: 0.695 (+/-0.047) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'invscaling', 'penalty': 'elasticnet', 'power_t': 0.5}\n\t[6085]: 0.695 (+/-0.047) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'invscaling', 'penalty': 'elasticnet', 'power_t': 0.6}\n\t[6086]: 0.695 (+/-0.047) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'invscaling', 'penalty': 'elasticnet', 'power_t': 0.7}\n\t[6087]: 0.695 (+/-0.047) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'invscaling', 'penalty': 'elasticnet', 'power_t': 0.8}\n\t[6088]: 0.790 (+/-0.205) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'invscaling', 'penalty': 'none', 'power_t': 0.1}\n\t[6089]: 0.886 (+/-0.196) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'invscaling', 'penalty': 'none', 'power_t': 0.2}\n\t[6090]: 0.886 (+/-0.196) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'invscaling', 'penalty': 'none', 'power_t': 0.3}\n\t[6091]: 0.876 (+/-0.097) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'invscaling', 'penalty': 'none', 'power_t': 0.4}\n\t[6092]: 0.829 (+/-0.129) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'invscaling', 'penalty': 'none', 'power_t': 0.5}\n\t[6093]: 0.743 (+/-0.129) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'invscaling', 'penalty': 'none', 'power_t': 0.6}\n\t[6094]: 0.695 (+/-0.047) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'invscaling', 'penalty': 'none', 'power_t': 0.7}\n\t[6095]: 0.695 (+/-0.047) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'invscaling', 'penalty': 'none', 'power_t': 0.8}\n\t[6096]: 0.714 (+/-0.104) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'invscaling', 'penalty': 'l1', 'power_t': 0.1}\n\t[6097]: 0.695 (+/-0.047) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'invscaling', 'penalty': 'l1', 'power_t': 0.2}\n\t[6098]: 0.724 (+/-0.093) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'invscaling', 'penalty': 'l1', 'power_t': 0.3}\n\t[6099]: 0.743 (+/-0.155) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'invscaling', 'penalty': 'l1', 'power_t': 0.4}\n\t[6100]: 0.705 (+/-0.071) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'invscaling', 'penalty': 'l1', 'power_t': 0.5}\n\t[6101]: 0.695 (+/-0.047) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'invscaling', 'penalty': 'l1', 'power_t': 0.6}\n\t[6102]: 0.695 (+/-0.047) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'invscaling', 'penalty': 'l1', 'power_t': 0.7}\n\t[6103]: 0.695 (+/-0.047) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'invscaling', 'penalty': 'l1', 'power_t': 0.8}\n\t[6104]: 0.695 (+/-0.047) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'invscaling', 'penalty': 'l2', 'power_t': 0.1}\n\t[6105]: 0.838 (+/-0.253) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'invscaling', 'penalty': 'l2', 'power_t': 0.2}\n\t[6106]: 0.752 (+/-0.203) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'invscaling', 'penalty': 'l2', 'power_t': 0.3}\n\t[6107]: 0.724 (+/-0.111) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'invscaling', 'penalty': 'l2', 'power_t': 0.4}\n\t[6108]: 0.752 (+/-0.140) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'invscaling', 'penalty': 'l2', 'power_t': 0.5}\n\t[6109]: 0.695 (+/-0.047) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'invscaling', 'penalty': 'l2', 'power_t': 0.6}\n\t[6110]: 0.695 (+/-0.047) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'invscaling', 'penalty': 'l2', 'power_t': 0.7}\n\t[6111]: 0.695 (+/-0.047) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'invscaling', 'penalty': 'l2', 'power_t': 0.8}\n\t[6112]: 0.771 (+/-0.093) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'elasticnet', 'power_t': 0.1}\n\t[6113]: 0.781 (+/-0.076) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'elasticnet', 'power_t': 0.2}\n\t[6114]: 0.781 (+/-0.047) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'elasticnet', 'power_t': 0.3}\n\t[6115]: 0.762 (+/-0.120) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'elasticnet', 'power_t': 0.4}\n\t[6116]: 0.762 (+/-0.085) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'elasticnet', 'power_t': 0.5}\n\t[6117]: 0.762 (+/-0.120) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'elasticnet', 'power_t': 0.6}\n\t[6118]: 0.771 (+/-0.071) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'elasticnet', 'power_t': 0.7}\n\t[6119]: 0.752 (+/-0.111) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'elasticnet', 'power_t': 0.8}\n\t[6120]: 0.971 (+/-0.047) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'none', 'power_t': 0.1}\n\t[6121]: 0.962 (+/-0.071) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'none', 'power_t': 0.2}\n\t[6122]: 0.971 (+/-0.047) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'none', 'power_t': 0.3}\n\t[6123]: 0.952 (+/-0.060) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'none', 'power_t': 0.4}\n\t[6124]: 0.962 (+/-0.071) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'none', 'power_t': 0.5}\n\t[6125]: 0.971 (+/-0.047) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'none', 'power_t': 0.6}\n\t[6126]: 0.971 (+/-0.047) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'none', 'power_t': 0.7}\n\t[6127]: 0.981 (+/-0.047) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'none', 'power_t': 0.8}\n\t[6128]: 0.790 (+/-0.114) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'l1', 'power_t': 0.1}\n\t[6129]: 0.762 (+/-0.135) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'l1', 'power_t': 0.2}\n\t[6130]: 0.733 (+/-0.097) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'l1', 'power_t': 0.3}\n\t[6131]: 0.752 (+/-0.126) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'l1', 'power_t': 0.4}\n\t[6132]: 0.762 (+/-0.104) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'l1', 'power_t': 0.5}\n\t[6133]: 0.781 (+/-0.097) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'l1', 'power_t': 0.6}\n\t[6134]: 0.762 (+/-0.085) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'l1', 'power_t': 0.7}\n\t[6135]: 0.743 (+/-0.097) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'l1', 'power_t': 0.8}\n\t[6136]: 0.790 (+/-0.076) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'l2', 'power_t': 0.1}\n\t[6137]: 0.800 (+/-0.071) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'l2', 'power_t': 0.2}\n\t[6138]: 0.800 (+/-0.071) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'l2', 'power_t': 0.3}\n\t[6139]: 0.819 (+/-0.111) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'l2', 'power_t': 0.4}\n\t[6140]: 0.838 (+/-0.097) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'l2', 'power_t': 0.5}\n\t[6141]: 0.829 (+/-0.129) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'l2', 'power_t': 0.6}\n\t[6142]: 0.810 (+/-0.060) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'l2', 'power_t': 0.7}\n\t[6143]: 0.829 (+/-0.076) for {'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'l2', 'power_t': 0.8}\n\nDetailed classification report:\n\tThe model is trained on the full development set.\n\tThe scores are computed on the full evaluation set.\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        16\n           1       1.00      0.89      0.94        18\n           2       0.85      1.00      0.92        11\n\n    accuracy                           0.96        45\n   macro avg       0.95      0.96      0.95        45\nweighted avg       0.96      0.96      0.96        45\n\n\nCTOR for best model: SGDClassifier(alpha=0.1, average=False, class_weight=None, early_stopping=False,\n              epsilon=0.1, eta0=0.1, fit_intercept=True, l1_ratio=0.001,\n              learning_rate='optimal', loss='hinge', max_iter=1000,\n              n_iter_no_change=5, n_jobs=None, penalty='none', power_t=0.3,\n              random_state=None, shuffle=True, tol=0.001,\n              validation_fraction=0.1, verbose=0, warm_start=False)\n\nbest: dat=iris, score=1.00000, model=SGDClassifier(alpha=0.1,eta0=0.1,l1_ratio=0.001,learning_rate='optimal',penalty='none',power_t=0.3)\n\nDone!\n"
    }
   ],
   "source": [
    "model = SGDClassifier()\n",
    "\n",
    "parameters = {\n",
    "    'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "    'power_t': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8],\n",
    "    'eta0': [0.0001, 0.001, 0.01, 0.1],\n",
    "    'penalty':['elasticnet','none', 'l1', 'l2'],\n",
    "    'alpha': [10**-5, 0.1],\n",
    "    'l1_ratio': [0.00001, 0.0001, 0.001, 0.01, 0.1, 0.5]\n",
    "}\n",
    "\n",
    "\n",
    "CV=5\n",
    "VERBOSE=0\n",
    "\n",
    "start = time()\n",
    "grid_tuned = GridSearchCV(model, parameters, cv=CV, scoring='f1_micro', verbose=VERBOSE, n_jobs=-1, iid=True)\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "t = time()-start\n",
    "\n",
    "b0, m0= FullReport(grid_tuned , X_test, y_test, t)\n",
    "\n",
    "print('Done!')\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qc Hyperparameter Random  Search using an SDG classifier\n",
    "\n",
    "Now, add code to run a `RandomizedSearchCV` instead.\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/F20_itmal/L09/Figs/randomsearch.png\" style=\"width:350px\">\n",
    "<small><em>\n",
    "    <center> Conceptual graphical view of randomized search for two distinct hyperparameters. </center> \n",
    "</em></small>\n",
    "\n",
    "Use these default parameters for the random search, similar to the default parameters for the grid search\n",
    "\n",
    "```python\n",
    "random_tuned = RandomizedSearchCV(\n",
    "    model, \n",
    "    tuning_parameters, \n",
    "    n_iter=20, \n",
    "    random_state=42, \n",
    "    cv=CV, \n",
    "    scoring='f1_micro', \n",
    "    verbose=VERBOSE, \n",
    "    n_jobs=-1, \n",
    "    iid=True\n",
    ")\n",
    "```\n",
    "\n",
    "but with the two new parameters, `n_iter` and `random_state` added. Since the search-type is now randow, the `random_state` gives sense, but essential to random search is the new `n_tier` parameter.\n",
    "\n",
    "So: investigate the `n_iter` parameter...in code and write an conceptual explanation  in text.\n",
    "\n",
    "Comparison of time (seconds) to complete `GridSearch` versus `RandomizedSearchCV`, does not necessarily give any sense, if your grid search completes in a few seconds (as for the iris tiny-data). You need a search that runs for minute, hours, or days.\n",
    "\n",
    "But you could compare the best-tuned parameter set and best scoring for the two methods. Is the random search best model close to the grid search?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": " 0.895 (+/-0.140) for {'power_t': 0.8, 'penalty': 'l1', 'learning_rate': 'constant', 'l1_ratio': 0.01, 'eta0': 0.01, 'alpha': 1e-05}\n\t[868]: 0.695 (+/-0.047) for {'power_t': 0.3, 'penalty': 'none', 'learning_rate': 'invscaling', 'l1_ratio': 0.5, 'eta0': 0.001, 'alpha': 0.1}\n\t[869]: 0.971 (+/-0.047) for {'power_t': 0.1, 'penalty': 'none', 'learning_rate': 'optimal', 'l1_ratio': 0.001, 'eta0': 0.001, 'alpha': 0.1}\n\t[870]: 0.962 (+/-0.038) for {'power_t': 0.7, 'penalty': 'none', 'learning_rate': 'optimal', 'l1_ratio': 1e-05, 'eta0': 0.001, 'alpha': 0.1}\n\t[871]: 0.714 (+/-0.104) for {'power_t': 0.7, 'penalty': 'elasticnet', 'learning_rate': 'adaptive', 'l1_ratio': 0.5, 'eta0': 0.001, 'alpha': 1e-05}\n\t[872]: 0.695 (+/-0.047) for {'power_t': 0.1, 'penalty': 'l1', 'learning_rate': 'adaptive', 'l1_ratio': 0.01, 'eta0': 0.0001, 'alpha': 1e-05}\n\t[873]: 0.819 (+/-0.164) for {'power_t': 0.8, 'penalty': 'l1', 'learning_rate': 'constant', 'l1_ratio': 0.1, 'eta0': 0.01, 'alpha': 1e-05}\n\t[874]: 0.810 (+/-0.135) for {'power_t': 0.2, 'penalty': 'l1', 'learning_rate': 'optimal', 'l1_ratio': 0.01, 'eta0': 0.1, 'alpha': 1e-05}\n\t[875]: 0.695 (+/-0.047) for {'power_t': 0.1, 'penalty': 'l1', 'learning_rate': 'constant', 'l1_ratio': 0.01, 'eta0': 0.001, 'alpha': 0.1}\n\t[876]: 0.886 (+/-0.230) for {'power_t': 0.3, 'penalty': 'l2', 'learning_rate': 'constant', 'l1_ratio': 0.01, 'eta0': 0.01, 'alpha': 1e-05}\n\t[877]: 0.914 (+/-0.038) for {'power_t': 0.8, 'penalty': 'l1', 'learning_rate': 'adaptive', 'l1_ratio': 0.0001, 'eta0': 0.01, 'alpha': 1e-05}\n\t[878]: 0.695 (+/-0.047) for {'power_t': 0.2, 'penalty': 'l1', 'learning_rate': 'constant', 'l1_ratio': 0.1, 'eta0': 0.0001, 'alpha': 1e-05}\n\t[879]: 0.981 (+/-0.047) for {'power_t': 0.5, 'penalty': 'none', 'learning_rate': 'adaptive', 'l1_ratio': 0.1, 'eta0': 0.1, 'alpha': 1e-05}\n\t[880]: 0.752 (+/-0.516) for {'power_t': 0.7, 'penalty': 'l2', 'learning_rate': 'optimal', 'l1_ratio': 0.001, 'eta0': 0.001, 'alpha': 1e-05}\n\t[881]: 0.695 (+/-0.047) for {'power_t': 0.5, 'penalty': 'l1', 'learning_rate': 'invscaling', 'l1_ratio': 0.01, 'eta0': 0.001, 'alpha': 0.1}\n\t[882]: 0.743 (+/-0.114) for {'power_t': 0.8, 'penalty': 'none', 'learning_rate': 'constant', 'l1_ratio': 0.001, 'eta0': 0.001, 'alpha': 0.1}\n\t[883]: 0.648 (+/-0.076) for {'power_t': 0.7, 'penalty': 'l1', 'learning_rate': 'optimal', 'l1_ratio': 0.001, 'eta0': 0.001, 'alpha': 1e-05}\n\t[884]: 0.695 (+/-0.047) for {'power_t': 0.2, 'penalty': 'l2', 'learning_rate': 'invscaling', 'l1_ratio': 0.001, 'eta0': 0.001, 'alpha': 1e-05}\n\t[885]: 0.695 (+/-0.047) for {'power_t': 0.5, 'penalty': 'l1', 'learning_rate': 'adaptive', 'l1_ratio': 1e-05, 'eta0': 0.0001, 'alpha': 1e-05}\n\t[886]: 0.971 (+/-0.047) for {'power_t': 0.7, 'penalty': 'none', 'learning_rate': 'optimal', 'l1_ratio': 0.01, 'eta0': 0.01, 'alpha': 0.1}\n\t[887]: 0.857 (+/-0.200) for {'power_t': 0.3, 'penalty': 'elasticnet', 'learning_rate': 'optimal', 'l1_ratio': 0.001, 'eta0': 0.0001, 'alpha': 1e-05}\n\t[888]: 0.371 (+/-0.038) for {'power_t': 0.3, 'penalty': 'l1', 'learning_rate': 'invscaling', 'l1_ratio': 0.5, 'eta0': 0.0001, 'alpha': 0.1}\n\t[889]: 0.695 (+/-0.047) for {'power_t': 0.7, 'penalty': 'l2', 'learning_rate': 'invscaling', 'l1_ratio': 1e-05, 'eta0': 0.01, 'alpha': 0.1}\n\t[890]: 0.790 (+/-0.196) for {'power_t': 0.6, 'penalty': 'elasticnet', 'learning_rate': 'optimal', 'l1_ratio': 1e-05, 'eta0': 0.1, 'alpha': 0.1}\n\t[891]: 0.867 (+/-0.220) for {'power_t': 0.5, 'penalty': 'none', 'learning_rate': 'constant', 'l1_ratio': 1e-05, 'eta0': 0.1, 'alpha': 0.1}\n\t[892]: 0.705 (+/-0.071) for {'power_t': 0.7, 'penalty': 'none', 'learning_rate': 'invscaling', 'l1_ratio': 0.001, 'eta0': 0.1, 'alpha': 1e-05}\n\t[893]: 0.695 (+/-0.047) for {'power_t': 0.5, 'penalty': 'none', 'learning_rate': 'invscaling', 'l1_ratio': 1e-05, 'eta0': 0.01, 'alpha': 1e-05}\n\t[894]: 0.771 (+/-0.152) for {'power_t': 0.5, 'penalty': 'l1', 'learning_rate': 'optimal', 'l1_ratio': 0.0001, 'eta0': 0.0001, 'alpha': 0.1}\n\t[895]: 0.714 (+/-0.395) for {'power_t': 0.1, 'penalty': 'elasticnet', 'learning_rate': 'optimal', 'l1_ratio': 0.01, 'eta0': 0.0001, 'alpha': 1e-05}\n\t[896]: 0.800 (+/-0.152) for {'power_t': 0.6, 'penalty': 'elasticnet', 'learning_rate': 'optimal', 'l1_ratio': 0.5, 'eta0': 0.001, 'alpha': 0.1}\n\t[897]: 0.695 (+/-0.047) for {'power_t': 0.3, 'penalty': 'l1', 'learning_rate': 'constant', 'l1_ratio': 0.1, 'eta0': 0.001, 'alpha': 0.1}\n\t[898]: 0.371 (+/-0.038) for {'power_t': 0.4, 'penalty': 'l1', 'learning_rate': 'invscaling', 'l1_ratio': 0.01, 'eta0': 0.0001, 'alpha': 1e-05}\n\t[899]: 0.743 (+/-0.299) for {'power_t': 0.4, 'penalty': 'l2', 'learning_rate': 'optimal', 'l1_ratio': 0.1, 'eta0': 0.001, 'alpha': 1e-05}\n\t[900]: 0.714 (+/-0.104) for {'power_t': 0.7, 'penalty': 'l1', 'learning_rate': 'constant', 'l1_ratio': 0.001, 'eta0': 0.01, 'alpha': 0.1}\n\t[901]: 0.648 (+/-0.143) for {'power_t': 0.4, 'penalty': 'l1', 'learning_rate': 'optimal', 'l1_ratio': 0.01, 'eta0': 0.1, 'alpha': 1e-05}\n\t[902]: 0.971 (+/-0.047) for {'power_t': 0.1, 'penalty': 'l1', 'learning_rate': 'adaptive', 'l1_ratio': 0.001, 'eta0': 0.1, 'alpha': 1e-05}\n\t[903]: 0.733 (+/-0.097) for {'power_t': 0.3, 'penalty': 'l2', 'learning_rate': 'constant', 'l1_ratio': 0.01, 'eta0': 0.001, 'alpha': 1e-05}\n\t[904]: 0.695 (+/-0.047) for {'power_t': 0.5, 'penalty': 'none', 'learning_rate': 'adaptive', 'l1_ratio': 0.1, 'eta0': 0.0001, 'alpha': 0.1}\n\t[905]: 0.695 (+/-0.047) for {'power_t': 0.1, 'penalty': 'l1', 'learning_rate': 'adaptive', 'l1_ratio': 1e-05, 'eta0': 0.0001, 'alpha': 1e-05}\n\t[906]: 0.695 (+/-0.047) for {'power_t': 0.4, 'penalty': 'l1', 'learning_rate': 'constant', 'l1_ratio': 0.0001, 'eta0': 0.001, 'alpha': 0.1}\n\t[907]: 0.695 (+/-0.047) for {'power_t': 0.2, 'penalty': 'l1', 'learning_rate': 'invscaling', 'l1_ratio': 0.5, 'eta0': 0.0001, 'alpha': 1e-05}\n\t[908]: 0.771 (+/-0.220) for {'power_t': 0.4, 'penalty': 'elasticnet', 'learning_rate': 'optimal', 'l1_ratio': 0.1, 'eta0': 0.001, 'alpha': 1e-05}\n\t[909]: 0.981 (+/-0.047) for {'power_t': 0.4, 'penalty': 'elasticnet', 'learning_rate': 'adaptive', 'l1_ratio': 0.1, 'eta0': 0.1, 'alpha': 1e-05}\n\t[910]: 0.971 (+/-0.047) for {'power_t': 0.7, 'penalty': 'none', 'learning_rate': 'adaptive', 'l1_ratio': 0.1, 'eta0': 0.1, 'alpha': 0.1}\n\t[911]: 0.810 (+/-0.241) for {'power_t': 0.1, 'penalty': 'l1', 'learning_rate': 'invscaling', 'l1_ratio': 0.5, 'eta0': 0.01, 'alpha': 1e-05}\n\t[912]: 0.914 (+/-0.071) for {'power_t': 0.8, 'penalty': 'elasticnet', 'learning_rate': 'adaptive', 'l1_ratio': 0.5, 'eta0': 0.01, 'alpha': 1e-05}\n\t[913]: 0.714 (+/-0.104) for {'power_t': 0.1, 'penalty': 'l1', 'learning_rate': 'invscaling', 'l1_ratio': 0.1, 'eta0': 0.001, 'alpha': 1e-05}\n\t[914]: 0.695 (+/-0.047) for {'power_t': 0.6, 'penalty': 'l2', 'learning_rate': 'invscaling', 'l1_ratio': 0.0001, 'eta0': 0.01, 'alpha': 0.1}\n\t[915]: 0.695 (+/-0.047) for {'power_t': 0.4, 'penalty': 'l1', 'learning_rate': 'invscaling', 'l1_ratio': 0.5, 'eta0': 0.001, 'alpha': 1e-05}\n\t[916]: 0.695 (+/-0.047) for {'power_t': 0.4, 'penalty': 'elasticnet', 'learning_rate': 'invscaling', 'l1_ratio': 0.1, 'eta0': 0.01, 'alpha': 0.1}\n\t[917]: 0.971 (+/-0.047) for {'power_t': 0.3, 'penalty': 'l2', 'learning_rate': 'adaptive', 'l1_ratio': 0.0001, 'eta0': 0.1, 'alpha': 1e-05}\n\t[918]: 0.790 (+/-0.230) for {'power_t': 0.3, 'penalty': 'l2', 'learning_rate': 'constant', 'l1_ratio': 1e-05, 'eta0': 0.01, 'alpha': 0.1}\n\t[919]: 0.743 (+/-0.214) for {'power_t': 0.6, 'penalty': 'l1', 'learning_rate': 'constant', 'l1_ratio': 0.01, 'eta0': 0.01, 'alpha': 0.1}\n\t[920]: 0.695 (+/-0.047) for {'power_t': 0.1, 'penalty': 'l2', 'learning_rate': 'constant', 'l1_ratio': 0.0001, 'eta0': 0.001, 'alpha': 0.1}\n\t[921]: 0.743 (+/-0.214) for {'power_t': 0.8, 'penalty': 'elasticnet', 'learning_rate': 'optimal', 'l1_ratio': 0.001, 'eta0': 0.001, 'alpha': 0.1}\n\t[922]: 0.733 (+/-0.097) for {'power_t': 0.3, 'penalty': 'elasticnet', 'learning_rate': 'constant', 'l1_ratio': 0.001, 'eta0': 0.001, 'alpha': 1e-05}\n\t[923]: 0.743 (+/-0.214) for {'power_t': 0.4, 'penalty': 'l2', 'learning_rate': 'constant', 'l1_ratio': 0.1, 'eta0': 0.01, 'alpha': 0.1}\n\t[924]: 0.819 (+/-0.236) for {'power_t': 0.3, 'penalty': 'none', 'learning_rate': 'constant', 'l1_ratio': 0.01, 'eta0': 0.01, 'alpha': 1e-05}\n\t[925]: 0.695 (+/-0.047) for {'power_t': 0.5, 'penalty': 'l2', 'learning_rate': 'invscaling', 'l1_ratio': 1e-05, 'eta0': 0.001, 'alpha': 0.1}\n\t[926]: 0.714 (+/-0.181) for {'power_t': 0.5, 'penalty': 'none', 'learning_rate': 'optimal', 'l1_ratio': 1e-05, 'eta0': 0.0001, 'alpha': 1e-05}\n\t[927]: 0.714 (+/-0.104) for {'power_t': 0.8, 'penalty': 'none', 'learning_rate': 'adaptive', 'l1_ratio': 0.001, 'eta0': 0.001, 'alpha': 1e-05}\n\t[928]: 0.695 (+/-0.047) for {'power_t': 0.3, 'penalty': 'l1', 'learning_rate': 'adaptive', 'l1_ratio': 0.0001, 'eta0': 0.0001, 'alpha': 1e-05}\n\t[929]: 0.733 (+/-0.155) for {'power_t': 0.2, 'penalty': 'elasticnet', 'learning_rate': 'invscaling', 'l1_ratio': 0.0001, 'eta0': 0.1, 'alpha': 0.1}\n\t[930]: 0.714 (+/-0.104) for {'power_t': 0.6, 'penalty': 'none', 'learning_rate': 'constant', 'l1_ratio': 0.1, 'eta0': 0.001, 'alpha': 1e-05}\n\t[931]: 0.667 (+/-0.104) for {'power_t': 0.6, 'penalty': 'elasticnet', 'learning_rate': 'constant', 'l1_ratio': 0.1, 'eta0': 0.1, 'alpha': 0.1}\n\t[932]: 0.695 (+/-0.047) for {'power_t': 0.5, 'penalty': 'l1', 'learning_rate': 'constant', 'l1_ratio': 0.01, 'eta0': 0.0001, 'alpha': 0.1}\n\t[933]: 0.714 (+/-0.241) for {'power_t': 0.7, 'penalty': 'l1', 'learning_rate': 'constant', 'l1_ratio': 0.5, 'eta0': 0.1, 'alpha': 0.1}\n\t[934]: 0.933 (+/-0.097) for {'power_t': 0.7, 'penalty': 'none', 'learning_rate': 'optimal', 'l1_ratio': 0.0001, 'eta0': 0.01, 'alpha': 0.1}\n\t[935]: 0.819 (+/-0.185) for {'power_t': 0.6, 'penalty': 'none', 'learning_rate': 'constant', 'l1_ratio': 0.01, 'eta0': 0.01, 'alpha': 1e-05}\n\t[936]: 0.895 (+/-0.194) for {'power_t': 0.2, 'penalty': 'elasticnet', 'learning_rate': 'invscaling', 'l1_ratio': 0.1, 'eta0': 0.1, 'alpha': 1e-05}\n\t[937]: 0.695 (+/-0.047) for {'power_t': 0.5, 'penalty': 'elasticnet', 'learning_rate': 'adaptive', 'l1_ratio': 0.01, 'eta0': 0.0001, 'alpha': 1e-05}\n\t[938]: 0.971 (+/-0.047) for {'power_t': 0.2, 'penalty': 'l2', 'learning_rate': 'adaptive', 'l1_ratio': 0.5, 'eta0': 0.1, 'alpha': 1e-05}\n\t[939]: 0.695 (+/-0.047) for {'power_t': 0.8, 'penalty': 'elasticnet', 'learning_rate': 'invscaling', 'l1_ratio': 0.1, 'eta0': 0.01, 'alpha': 1e-05}\n\t[940]: 0.981 (+/-0.047) for {'power_t': 0.6, 'penalty': 'l2', 'learning_rate': 'adaptive', 'l1_ratio': 0.001, 'eta0': 0.1, 'alpha': 1e-05}\n\t[941]: 0.371 (+/-0.038) for {'power_t': 0.7, 'penalty': 'none', 'learning_rate': 'invscaling', 'l1_ratio': 0.01, 'eta0': 0.001, 'alpha': 1e-05}\n\t[942]: 0.695 (+/-0.047) for {'power_t': 0.6, 'penalty': 'l2', 'learning_rate': 'adaptive', 'l1_ratio': 1e-05, 'eta0': 0.001, 'alpha': 0.1}\n\t[943]: 0.829 (+/-0.238) for {'power_t': 0.7, 'penalty': 'elasticnet', 'learning_rate': 'optimal', 'l1_ratio': 0.5, 'eta0': 0.001, 'alpha': 1e-05}\n\t[944]: 0.895 (+/-0.071) for {'power_t': 0.4, 'penalty': 'l2', 'learning_rate': 'adaptive', 'l1_ratio': 1e-05, 'eta0': 0.01, 'alpha': 1e-05}\n\t[945]: 0.810 (+/-0.120) for {'power_t': 0.2, 'penalty': 'elasticnet', 'learning_rate': 'adaptive', 'l1_ratio': 0.01, 'eta0': 0.1, 'alpha': 0.1}\n\t[946]: 0.829 (+/-0.267) for {'power_t': 0.1, 'penalty': 'elasticnet', 'learning_rate': 'optimal', 'l1_ratio': 0.5, 'eta0': 0.01, 'alpha': 1e-05}\n\t[947]: 0.695 (+/-0.047) for {'power_t': 0.5, 'penalty': 'l2', 'learning_rate': 'constant', 'l1_ratio': 0.0001, 'eta0': 0.0001, 'alpha': 1e-05}\n\t[948]: 0.819 (+/-0.164) for {'power_t': 0.2, 'penalty': 'none', 'learning_rate': 'optimal', 'l1_ratio': 0.001, 'eta0': 0.1, 'alpha': 1e-05}\n\t[949]: 0.895 (+/-0.111) for {'power_t': 0.4, 'penalty': 'l2', 'learning_rate': 'invscaling', 'l1_ratio': 0.001, 'eta0': 0.1, 'alpha': 1e-05}\n\t[950]: 0.762 (+/-0.209) for {'power_t': 0.1, 'penalty': 'l2', 'learning_rate': 'constant', 'l1_ratio': 0.0001, 'eta0': 0.1, 'alpha': 1e-05}\n\t[951]: 0.962 (+/-0.071) for {'power_t': 0.7, 'penalty': 'none', 'learning_rate': 'optimal', 'l1_ratio': 0.001, 'eta0': 0.0001, 'alpha': 0.1}\n\t[952]: 0.733 (+/-0.477) for {'power_t': 0.3, 'penalty': 'elasticnet', 'learning_rate': 'optimal', 'l1_ratio': 0.0001, 'eta0': 0.001, 'alpha': 1e-05}\n\t[953]: 0.733 (+/-0.047) for {'power_t': 0.2, 'penalty': 'none', 'learning_rate': 'optimal', 'l1_ratio': 0.01, 'eta0': 0.01, 'alpha': 1e-05}\n\t[954]: 0.990 (+/-0.038) for {'power_t': 0.7, 'penalty': 'l1', 'learning_rate': 'adaptive', 'l1_ratio': 0.0001, 'eta0': 0.1, 'alpha': 1e-05}\n\t[955]: 0.695 (+/-0.047) for {'power_t': 0.3, 'penalty': 'l2', 'learning_rate': 'constant', 'l1_ratio': 0.01, 'eta0': 0.0001, 'alpha': 1e-05}\n\t[956]: 0.771 (+/-0.152) for {'power_t': 0.4, 'penalty': 'elasticnet', 'learning_rate': 'constant', 'l1_ratio': 0.01, 'eta0': 0.001, 'alpha': 1e-05}\n\t[957]: 0.781 (+/-0.196) for {'power_t': 0.4, 'penalty': 'elasticnet', 'learning_rate': 'optimal', 'l1_ratio': 0.5, 'eta0': 0.001, 'alpha': 0.1}\n\t[958]: 0.371 (+/-0.038) for {'power_t': 0.8, 'penalty': 'elasticnet', 'learning_rate': 'invscaling', 'l1_ratio': 0.01, 'eta0': 0.001, 'alpha': 0.1}\n\t[959]: 0.981 (+/-0.047) for {'power_t': 0.3, 'penalty': 'none', 'learning_rate': 'optimal', 'l1_ratio': 1e-05, 'eta0': 0.001, 'alpha': 0.1}\n\t[960]: 0.695 (+/-0.047) for {'power_t': 0.6, 'penalty': 'none', 'learning_rate': 'constant', 'l1_ratio': 1e-05, 'eta0': 0.0001, 'alpha': 0.1}\n\t[961]: 0.971 (+/-0.047) for {'power_t': 0.3, 'penalty': 'none', 'learning_rate': 'adaptive', 'l1_ratio': 0.5, 'eta0': 0.1, 'alpha': 1e-05}\n\t[962]: 0.771 (+/-0.126) for {'power_t': 0.3, 'penalty': 'l2', 'learning_rate': 'adaptive', 'l1_ratio': 0.0001, 'eta0': 0.01, 'alpha': 0.1}\n\t[963]: 0.714 (+/-0.085) for {'power_t': 0.8, 'penalty': 'l2', 'learning_rate': 'adaptive', 'l1_ratio': 0.001, 'eta0': 0.01, 'alpha': 0.1}\n\t[964]: 0.848 (+/-0.203) for {'power_t': 0.1, 'penalty': 'l2', 'learning_rate': 'optimal', 'l1_ratio': 1e-05, 'eta0': 0.001, 'alpha': 1e-05}\n\t[965]: 0.724 (+/-0.152) for {'power_t': 0.8, 'penalty': 'l2', 'learning_rate': 'optimal', 'l1_ratio': 0.01, 'eta0': 0.01, 'alpha': 1e-05}\n\t[966]: 0.848 (+/-0.194) for {'power_t': 0.1, 'penalty': 'l1', 'learning_rate': 'optimal', 'l1_ratio': 0.001, 'eta0': 0.0001, 'alpha': 0.1}\n\t[967]: 0.743 (+/-0.076) for {'power_t': 0.7, 'penalty': 'l1', 'learning_rate': 'optimal', 'l1_ratio': 0.5, 'eta0': 0.1, 'alpha': 0.1}\n\t[968]: 0.695 (+/-0.047) for {'power_t': 0.2, 'penalty': 'l2', 'learning_rate': 'adaptive', 'l1_ratio': 0.5, 'eta0': 0.001, 'alpha': 0.1}\n\t[969]: 0.676 (+/-0.251) for {'power_t': 0.2, 'penalty': 'l1', 'learning_rate': 'invscaling', 'l1_ratio': 0.1, 'eta0': 0.1, 'alpha': 0.1}\n\t[970]: 0.905 (+/-0.060) for {'power_t': 0.2, 'penalty': 'l2', 'learning_rate': 'adaptive', 'l1_ratio': 0.5, 'eta0': 0.01, 'alpha': 1e-05}\n\t[971]: 0.829 (+/-0.205) for {'power_t': 0.8, 'penalty': 'l1', 'learning_rate': 'optimal', 'l1_ratio': 0.001, 'eta0': 0.01, 'alpha': 0.1}\n\t[972]: 0.695 (+/-0.047) for {'power_t': 0.2, 'penalty': 'none', 'learning_rate': 'invscaling', 'l1_ratio': 0.5, 'eta0': 0.0001, 'alpha': 1e-05}\n\t[973]: 0.771 (+/-0.126) for {'power_t': 0.4, 'penalty': 'l2', 'learning_rate': 'adaptive', 'l1_ratio': 0.0001, 'eta0': 0.01, 'alpha': 0.1}\n\t[974]: 0.724 (+/-0.111) for {'power_t': 0.3, 'penalty': 'none', 'learning_rate': 'constant', 'l1_ratio': 0.0001, 'eta0': 0.001, 'alpha': 1e-05}\n\t[975]: 0.771 (+/-0.152) for {'power_t': 0.6, 'penalty': 'l2', 'learning_rate': 'optimal', 'l1_ratio': 0.01, 'eta0': 0.01, 'alpha': 0.1}\n\t[976]: 0.752 (+/-0.111) for {'power_t': 0.4, 'penalty': 'l1', 'learning_rate': 'optimal', 'l1_ratio': 0.1, 'eta0': 0.01, 'alpha': 0.1}\n\t[977]: 0.743 (+/-0.155) for {'power_t': 0.7, 'penalty': 'none', 'learning_rate': 'adaptive', 'l1_ratio': 0.001, 'eta0': 0.001, 'alpha': 0.1}\n\t[978]: 0.781 (+/-0.143) for {'power_t': 0.8, 'penalty': 'l2', 'learning_rate': 'optimal', 'l1_ratio': 0.01, 'eta0': 0.001, 'alpha': 1e-05}\n\t[979]: 0.695 (+/-0.047) for {'power_t': 0.1, 'penalty': 'l1', 'learning_rate': 'constant', 'l1_ratio': 0.5, 'eta0': 0.001, 'alpha': 0.1}\n\t[980]: 0.629 (+/-0.298) for {'power_t': 0.3, 'penalty': 'l2', 'learning_rate': 'invscaling', 'l1_ratio': 0.5, 'eta0': 0.0001, 'alpha': 0.1}\n\t[981]: 0.695 (+/-0.047) for {'power_t': 0.4, 'penalty': 'elasticnet', 'learning_rate': 'adaptive', 'l1_ratio': 0.1, 'eta0': 0.0001, 'alpha': 1e-05}\n\t[982]: 0.714 (+/-0.217) for {'power_t': 0.3, 'penalty': 'l1', 'learning_rate': 'optimal', 'l1_ratio': 1e-05, 'eta0': 0.001, 'alpha': 1e-05}\n\t[983]: 0.743 (+/-0.129) for {'power_t': 0.4, 'penalty': 'l2', 'learning_rate': 'adaptive', 'l1_ratio': 0.1, 'eta0': 0.001, 'alpha': 1e-05}\n\t[984]: 0.981 (+/-0.047) for {'power_t': 0.4, 'penalty': 'elasticnet', 'learning_rate': 'adaptive', 'l1_ratio': 0.0001, 'eta0': 0.1, 'alpha': 1e-05}\n\t[985]: 0.800 (+/-0.229) for {'power_t': 0.4, 'penalty': 'elasticnet', 'learning_rate': 'constant', 'l1_ratio': 1e-05, 'eta0': 0.01, 'alpha': 0.1}\n\t[986]: 0.857 (+/-0.263) for {'power_t': 0.5, 'penalty': 'none', 'learning_rate': 'constant', 'l1_ratio': 0.001, 'eta0': 0.1, 'alpha': 0.1}\n\t[987]: 0.695 (+/-0.047) for {'power_t': 0.8, 'penalty': 'l2', 'learning_rate': 'constant', 'l1_ratio': 0.1, 'eta0': 0.001, 'alpha': 0.1}\n\t[988]: 0.876 (+/-0.245) for {'power_t': 0.8, 'penalty': 'none', 'learning_rate': 'constant', 'l1_ratio': 0.0001, 'eta0': 0.01, 'alpha': 1e-05}\n\t[989]: 0.695 (+/-0.047) for {'power_t': 0.3, 'penalty': 'l1', 'learning_rate': 'constant', 'l1_ratio': 0.1, 'eta0': 0.0001, 'alpha': 1e-05}\n\t[990]: 0.695 (+/-0.047) for {'power_t': 0.5, 'penalty': 'elasticnet', 'learning_rate': 'adaptive', 'l1_ratio': 0.5, 'eta0': 0.0001, 'alpha': 1e-05}\n\t[991]: 0.781 (+/-0.214) for {'power_t': 0.8, 'penalty': 'l1', 'learning_rate': 'optimal', 'l1_ratio': 0.5, 'eta0': 0.1, 'alpha': 1e-05}\n\t[992]: 0.667 (+/-0.351) for {'power_t': 0.6, 'penalty': 'l1', 'learning_rate': 'constant', 'l1_ratio': 0.001, 'eta0': 0.1, 'alpha': 1e-05}\n\t[993]: 0.695 (+/-0.047) for {'power_t': 0.7, 'penalty': 'l1', 'learning_rate': 'adaptive', 'l1_ratio': 0.5, 'eta0': 0.0001, 'alpha': 0.1}\n\t[994]: 0.848 (+/-0.185) for {'power_t': 0.6, 'penalty': 'l2', 'learning_rate': 'optimal', 'l1_ratio': 0.1, 'eta0': 0.1, 'alpha': 0.1}\n\t[995]: 0.752 (+/-0.111) for {'power_t': 0.6, 'penalty': 'none', 'learning_rate': 'invscaling', 'l1_ratio': 0.5, 'eta0': 0.1, 'alpha': 0.1}\n\t[996]: 0.829 (+/-0.230) for {'power_t': 0.7, 'penalty': 'elasticnet', 'learning_rate': 'constant', 'l1_ratio': 0.01, 'eta0': 0.1, 'alpha': 1e-05}\n\t[997]: 0.790 (+/-0.114) for {'power_t': 0.6, 'penalty': 'l1', 'learning_rate': 'invscaling', 'l1_ratio': 0.001, 'eta0': 0.1, 'alpha': 1e-05}\n\t[998]: 0.762 (+/-0.135) for {'power_t': 0.2, 'penalty': 'none', 'learning_rate': 'adaptive', 'l1_ratio': 0.5, 'eta0': 0.001, 'alpha': 1e-05}\n\t[999]: 0.695 (+/-0.047) for {'power_t': 0.8, 'penalty': 'none', 'learning_rate': 'invscaling', 'l1_ratio': 0.001, 'eta0': 0.1, 'alpha': 1e-05}\n\nDetailed classification report:\n\tThe model is trained on the full development set.\n\tThe scores are computed on the full evaluation set.\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        16\n           1       1.00      0.89      0.94        18\n           2       0.85      1.00      0.92        11\n\n    accuracy                           0.96        45\n   macro avg       0.95      0.96      0.95        45\nweighted avg       0.96      0.96      0.96        45\n\n\nCTOR for best model: SGDClassifier(alpha=0.1, average=False, class_weight=None, early_stopping=False,\n              epsilon=0.1, eta0=0.1, fit_intercept=True, l1_ratio=0.01,\n              learning_rate='optimal', loss='hinge', max_iter=1000,\n              n_iter_no_change=5, n_jobs=None, penalty='none', power_t=0.3,\n              random_state=None, shuffle=True, tol=0.001,\n              validation_fraction=0.1, verbose=0, warm_start=False)\n\nbest: dat=iris, score=0.99048, model=SGDClassifier(alpha=0.1,eta0=0.1,l1_ratio=0.01,learning_rate='optimal',penalty='none',power_t=0.3)\n\nOK\n"
    }
   ],
   "source": [
    "model = SGDClassifier()\n",
    "start = time()\n",
    "grid_tuned = RandomizedSearchCV(model, parameters, cv=CV, scoring='f1_micro', verbose=VERBOSE, n_jobs=-1, iid=True, n_iter=1000)\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "t = time()-start\n",
    "\n",
    "# Report result\n",
    "b0, m0= FullReport(grid_tuned , X_test, y_test, t)\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qd MNIST Search Quest II\n",
    "\n",
    "Finally, we create yet a search-quest competition: __who can find the best model+hyperparameters for MNIST dataset?__\n",
    "\n",
    "You change to the MNIST data by calling `LoadAndSetupData('mnist')`, and this is a completely other ball-game that the _tiny-data_ iris: it's much larger (but still far from _big-data_)!\n",
    "\n",
    "* You might opt for the exhaustive grid search, or use the faster but-less optimal random search...your choice. \n",
    "\n",
    "* You are free to pick any classifier in Scikit-learn, even algorithms we have not discussed yet---__except Neural Networks!__. \n",
    "\n",
    "* Keep the score function at `f1_micro`, otherwise, we will be comparing 'æbler og pærer'. \n",
    "\n",
    "* And, you may also want to scale you input data for some models to perform better.\n",
    "\n",
    "* __REMEMBER__, DO NOT USE any Neural Network models. This also means no to use any `Keras` or `Tensorflow` models...since they outperform most other models, and there are also too many examples on the net to cut-and-paste from!\n",
    "\n",
    "Check your result by printing the first _return_ value from `FullReport()` \n",
    "```python \n",
    "b1, m1 = FullReport(random_tuned , X_test, y_test, time_randomsearch)\n",
    "print(b1)\n",
    "```\n",
    "that will display a result like\n",
    "```\n",
    "best: dat=mnist, score=0.90780, model=SGDClassifier(alpha=1.0,eta0=0.0001,learning_rate='invscaling')\n",
    "```\n",
    "and paste your currently best model into the message box, for ITMAL group 09 like\n",
    "```\n",
    "Grp09: best: dat=mnist, score=0.90780, model=SGDClassifier(alpha=1.0,eta0=0.0001,learning_rate='invscaling')\n",
    "\n",
    "Grp09: CTOR for best model: SGDClassifier(alpha=1.0, average=False, class_weight=None, early_stopping=False,\n",
    "              epsilon=0.1, eta0=0.0001, fit_intercept=True, l1_ratio=0.15,\n",
    "              learning_rate='invscaling', loss='hinge', max_iter=1000,\n",
    "              n_iter_no_change=5, n_jobs=None, penalty='l2', power_t=0.5,\n",
    "              random_state=None, shuffle=True, tol=0.001,\n",
    "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "```\n",
    "              \n",
    "on Blackboard: \"L09: Optimering og søgning\" | \"Qd MNIST Search Quest II\"\n",
    "\n",
    "> https://blackboard.au.dk/webapps/blackboard/content/listContentEditable.jsp?content_id=_2485153_1&course_id=_134254_1&mode=reset#search_quest_ii\n",
    "\n",
    "and, check if your score (for MNIST) is better that the currently best score. Republish if you get a better score than your own previously best.\n",
    "\n",
    "Remember to provide a ITMAL group name manually, so we can identify a winnner: the 1.st price is yet a cake (well, not possible anymore due to COVID-19)! \n",
    "\n",
    "For the handin, report your progress in scoring choosing different models, hyperparameters to search and how you might need to preprocess your data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "DATA: mnist..\nUsing TensorFlow backend.\n  org. data:  X.shape      =(70000;  784), y.shape      =(70000)\n  train data: X_train.shape=(49000;  784), y_train.shape=(49000)\n  test data:  X_test.shape =(21000;  784), y_test.shape =(21000)\n\n"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = LoadAndSetupData('mnist') # 'iris', 'moon', or 'mnist'pprint('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-73c8b5a598a9>, line 12)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-73c8b5a598a9>\"\u001b[1;36m, line \u001b[1;32m12\u001b[0m\n\u001b[1;33m    50mm\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "\n",
    "######### KNeighborsClassifier  #########33##\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'algorithm':['ball_tree', 'kd_tree'],\n",
    "    'leaf_size': [15, 30, 45],\n",
    "    'n_neighbors':[3, 5, 7, 9, 11],\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "CV=5\n",
    "VERBOSE=20\n",
    "\n",
    "# Run GridSearchCV for the model\n",
    "start = time()\n",
    "grid_tuned = RandomizedSearchCV(model, parameters, cv=CV, scoring='f1_micro', verbose=VERBOSE, n_jobs=-1, iid=True, n_iter=10)\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "t = time()-start\n",
    "\n",
    "# Report result\n",
    "b0, m0= FullReport(grid_tuned , X_test, y_test, t)\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}